{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Flow Tutorial\n",
    "\n",
    "## MNIST For ML Beginners\n",
    "If you're new to machine learning, we recommend starting here. You'll learn about a classic problem, handwritten digit classification (MNIST), and get a gentle introduction to multiclass classification.\n",
    "\n",
    "[View Tutorial](MNIST/MNIST_For_ML_Beginners.ipynb)\n",
    "\n",
    "## Deep MNIST for Experts\n",
    "\n",
    "If you're already familiar with other deep learning software packages, and are already familiar with MNIST, this tutorial with give you a very brief primer on TensorFlow.\n",
    "\n",
    "[View Tutorial](MNIST/Deep_MNIST_for_Experts.ipynb)\n",
    "\n",
    "## TensorFlow Mechanics 101\n",
    "\n",
    "This is a technical tutorial, where we walk you through the details of using TensorFlow infrastructure to train models at scale. We use again MNIST as the example.\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "An introduction to convolutional neural networks using the CIFAR-10 data set. Convolutional neural nets are particularly tailored to images, since they exploit translation invariance to yield more compact and effective representations of visual content.\n",
    "\n",
    "[View Tutorial](Convolutional_Neural_Networks/Convolutional_Neural_Networks.ipynb)\n",
    "\n",
    "## Vector Representations of Words\n",
    "\n",
    "This tutorial motivates why it is useful to learn to represent words as vectors (called word embeddings). It introduces the word2vec model as an efficient method for learning embeddings. It also covers the high-level details behind noise-contrastive training methods (the biggest recent advance in training embeddings).\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "## Recurrent Neural Networks\n",
    "\n",
    "An introduction to RNNs, wherein we train an LSTM network to predict the next word in an English sentence. (A task sometimes called language modeling.)\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "## Sequence-to-Sequence Models\n",
    "\n",
    "A follow on to the RNN tutorial, where we assemble a sequence-to-sequence model for machine translation. You will learn to build your own English-to-French translator, entirely machine learned, end-to-end.\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "## Mandelbrot Set\n",
    "\n",
    "TensorFlow can be used for computation that has nothing to do with machine learning. Here's a naive implementation of Mandelbrot set visualization.\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "## Partial Differential Equations\n",
    "\n",
    "As another example of non-machine learning computation, we offer an example of a naive PDE simulation of raindrops landing on a pond.\n",
    "\n",
    "View Tutorial\n",
    "\n",
    "# MNIST Data Download\n",
    "\n",
    "Details about downloading the MNIST handwritten digits data set. Exciting stuff.\n",
    "\n",
    "View Tutorial\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
